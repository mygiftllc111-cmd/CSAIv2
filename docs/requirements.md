# Hospitality Agent Infrastructure - 要件定義書

## 要件定義の作成原則

- **「あったらいいな」は絶対に作らない**
- **拡張可能性のための余分な要素は一切追加しない**
- **将来の「もしかして」のための準備は禁止**
- **今、ここで必要な最小限の要素のみ**

## 1. プロジェクト概要

### 1.1 成果目標

新人研修チーム（毎月20名、常時最大60名）が、Notion/Googleドライブ上の社内ナレッジに対して、心理的安全性を保ったまま安心して質問・確認できるAIエージェント（社内CS版MVP）を構築する。

**ビジョン**: 「世界一賢いAI」ではなく「世界一安心して話せるAI」。ホスピタリティという概念をコードと仕様に落とし込み、技術で再現可能にする。

### 1.2 成功指標

#### 定量的指標

- 利用率: 新人研修チームの週次アクティブ利用率 70%以上
- 回答精度: ナレッジベースに基づく回答適合率 80%以上
- 対応速度: 質問から回答表示まで 5秒以内
- エスカレーション率: AIで解決できず人に聞き直す率を 30%以下に
- リピート率: 一度使ったユーザーが翌週も利用する率 60%以上

#### 定性的指標

- 新人が「聞いてよかった」「安心した」と感じる体験が生まれる
- 研修担当者への同じ質問の繰り返しが体感的に減少する
- 「怒られるかも」という不安なく質問できる場として認識される
- マニュアルを探す前にまずエージェントに聞く習慣が定着する
- 対話後に「次に何をすべきか」が明確になっている

## 2. システム全体像

### 2.1 主要機能一覧

- **対話機能**: テキスト/音声入力による質問、社内ナレッジに基づく安心感のある回答生成
- **ナレッジ連携**: Notion/Googleドライブからの自動取得・同期・RAG検索
- **対話ログ管理**: 全対話の完全保存、利用者属性別の分析・閲覧
- **管理機能**: プロンプト調整、ナレッジソース設定、ログ分析

### 2.2 「安心感」の実装仕様（必須機能）

このシステムの最大の差別化要素。UXではなく必須機能として実装する。

| # | 原則 | 実装内容 |
|---|------|---------|
| 01 | **否定しない（Do Not Deny）** | ユーザーの入力がいかに曖昧でも、まずは受け入れる姿勢をプロンプトで強制する |
| 02 | **共感を先に返す（Empathy First）** | いきなり回答を出さず、「それはお困りですね」「確認しますね」というクッション言葉を生成する |
| 03 | **断定しない（No Absolutes）** | ハルシネーション対策も含め、「〜です」と言い切るよりも「〜の可能性があります」と寄り添う表現を選ぶ |
| 04 | **Next Action** | 「わかりません」で終わらせず、必ず「次は誰に聞くべきか」「どこを見るべきか」の道筋を示す |

### 2.3 行動制約（NO ACTION原則）

- 独自の判断や、システムへの自動操作（Write処理）は行わない
- あくまで「検索と回答」に留める
- AIに最終決定権（決済、承認など）は持たせない

### 2.4 推論仕様

| 項目 | 仕様 |
|------|------|
| Intent Guessing | 単なるキーワードマッチではなく、「なぜその質問をしたのか？」という背景意図をLLMに推測させる |
| Context Window | 会話の文脈（ラリー）を保持し、前の発言を踏まえた回答を生成する |
| Constraint | 聞き返す（追加質問する）のは「1回」までという制約を設ける |

### 2.5 ユーザーロールと権限

| 機能 | 未承認利用者 | 承認済み利用者（新人） | 管理者 |
|------|-----------|-------------------|--------|
| 利用申請（プロフィール入力） | ○ | × | × |
| 承認待ち画面の表示 | ○ | × | × |
| チャット画面アクセス | × | ○ | ○ |
| 音声/テキストで質問 | × | ○ | ○ |
| 自分の対話履歴を見る | × | ○ | ○ |
| 利用者の承認・拒否 | × | × | ○ |
| 全対話ログの閲覧・分析 | × | × | ○ |
| プロンプト設定の変更 | × | × | ○ |
| ナレッジソース設定 | × | × | ○ |
| 管理画面アクセス | × | × | ○ |

### 2.6 認証・認可要件

```yaml
認証方式:
  利用者:
    - 限定公開URLにアクセス
    - 初回プロフィール入力（名前・所属・入社時期）
    - 管理者が入社リストと照合し承認
    - 承認後にチャット利用が可能になる
    - ブラウザのLocalStorageに認証トークンを記憶（再入力不要）
  管理者: パスワード認証

利用者ステータス:
  - pending（承認待ち）: プロフィール入力済み、管理者の承認待ち
  - approved（承認済み）: チャット利用可能
  - rejected（拒否）: アクセス不可

利用者識別:
  - 初回アクセス時に名前・所属・入社時期を入力
  - 管理者が入社リストと照合して承認
  - 承認後、対話ログに利用者属性を紐付け

セキュリティレベル:
  - 一般的な業務情報（マニュアル・手順書レベル）
  - 限定公開URL + 管理者承認制（二重のアクセス制御）
```

## 3. ページ詳細仕様

### 3.1 P-001: チャット画面

#### 目的

新人スタッフが社内ナレッジに対して安心して質問でき、「話してよかった」と感じる対話体験を提供する。

#### 主要機能

- 初回利用申請フォーム（名前・所属・入社時期）
- 承認待ち画面（申請後、管理者の承認を待つ状態）
- テキスト入力による質問（承認済みユーザーのみ）
- 音声入力による質問（マイクボタン切替、承認済みユーザーのみ）
- AIエージェントとのリアルタイム対話表示
- 対話履歴一覧（過去の会話を振り返り可能）
- 新規会話の開始

#### 必要な操作

| 操作種別 | 操作内容 | 必要な入力 | 期待される出力 |
|---------|---------|-----------|-------------|
| 作成 | 利用申請（プロフィール送信） | 名前、所属、入社時期 | 申請完了、承認待ち画面表示 |
| 取得 | 承認ステータス確認 | 利用者トークン | 承認済み→チャット画面 / 未承認→承認待ち画面 |
| 作成 | テキスト質問送信 | 質問テキスト | AIエージェントの回答（安心感のあるトーン） |
| 作成 | 音声質問送信 | 音声入力 | 音声→テキスト変換→AIエージェントの回答 |
| 取得 | 対話履歴一覧取得 | 利用者ID | 過去の会話リスト（日時・最初の質問） |
| 取得 | 対話詳細取得 | 会話ID | 選択した会話の全メッセージ |
| 作成 | 新規会話開始 | なし | 空のチャット画面 |

#### 処理フロー

1. 利用者が限定URLにアクセス
2. 初回の場合：利用申請フォームを表示（名前・所属・入社時期）
3. 申請送信 → 承認待ち画面を表示（「管理者の承認をお待ちください」）
4. 管理者がA-002で入社リストと照合し、承認/拒否
5. 承認済みの場合：チャット画面にアクセス可能（ブラウザに認証トークン保存）
6. 2回目以降：トークンで自動認証、承認済みなら直接チャット画面へ
7. テキストまたは音声で質問を入力
8. （音声の場合）Web Speech API / Whisper APIでテキストに変換
9. バックエンドがナレッジベースをRAG検索
10. LLMが安心感のある回答を生成（Empathy First → 回答 → Next Action）
11. 回答をチャット画面にリアルタイム表示
12. 対話ログをDBに保存（利用者属性 + 質問 + 回答 + タイムスタンプ）

#### データ構造（概念）

```yaml
UserProfile:
  識別子: profile_id（UUID、サーバー生成）
  基本情報:
    - name（必須）
    - department（必須）
    - join_date（必須、年月）
  承認情報:
    - status（pending / approved / rejected）
    - approved_at（承認日時、nullable）
    - auth_token（承認後に発行、UUID）
  メタ情報:
    - created_at
    - last_accessed_at

Conversation:
  識別子: conversation_id（UUID）
  基本情報:
    - title（最初の質問から自動生成）
  メタ情報:
    - created_at
    - updated_at
  関連:
    - UserProfile（多対1）
    - Message（1対多）

Message:
  識別子: message_id（UUID）
  基本情報:
    - role（user / assistant）
    - content（テキスト）
    - input_method（text / voice）
  メタ情報:
    - created_at
  関連:
    - Conversation（多対1）
```

### 3.2 A-001: 管理者ログイン

#### 目的

管理画面への不正アクセスを防止し、管理者のみが設定・分析機能にアクセスできるようにする。

#### 主要機能

- パスワード入力フォーム
- 認証成功時に管理画面へ遷移
- 認証失敗時のエラー表示

#### 必要な操作

| 操作種別 | 操作内容 | 必要な入力 | 期待される出力 |
|---------|---------|-----------|-------------|
| 取得 | 認証 | パスワード | 認証成功→管理画面遷移 / 認証失敗→エラー表示 |

#### 処理フロー

1. 管理者が管理画面URLにアクセス
2. パスワード入力画面を表示
3. パスワードを入力して送信
4. バックエンドで検証
5. 成功：管理画面（A-002）へ遷移、セッション発行
6. 失敗：エラーメッセージ表示

### 3.3 A-002: 管理画面

#### 目的

エージェントの回答品質を継続的に改善するため、対話ログの分析、プロンプトの調整、ナレッジソースの管理を一元的に行う。

#### 主要機能（4タブ構成）

**タブ1: 利用者承認**
- 新規申請一覧（名前・所属・入社時期・申請日時）
- 入社リストとの照合用情報表示
- 承認/拒否ボタン
- 承認済み利用者一覧（ステータス管理）

**タブ2: 対話ログ分析**
- 全利用者の対話履歴一覧（日時・利用者名・所属・入社時期・質問概要）
- フィルタ機能（所属別、入社時期別、期間指定）
- 対話詳細の展開表示（質問と回答の全文）
- よくある質問の集計・傾向分析

**タブ3: プロンプト設定**
- システムプロンプトの編集・保存
- 安心感トーンのFew-shot例の管理（追加・編集・削除）
- プロンプトのバージョン管理（変更履歴）

**タブ4: ナレッジソース設定**
- Notion接続設定（Integration Token、対象ページ/DB指定）
- Google Drive接続設定（サービスアカウント、対象フォルダ指定）
- 同期状況の表示（最終同期日時、ドキュメント数）
- 手動再同期ボタン
- 自動同期間隔の設定

#### 必要な操作

| 操作種別 | 操作内容 | 必要な入力 | 期待される出力 |
|---------|---------|-----------|-------------|
| 取得 | 承認待ち利用者一覧取得 | なし | 新規申請リスト（名前・所属・入社時期・申請日時） |
| 更新 | 利用者の承認 | 利用者ID | ステータスをapprovedに変更、認証トークン発行 |
| 更新 | 利用者の拒否 | 利用者ID | ステータスをrejectedに変更 |
| 取得 | 承認済み利用者一覧取得 | なし | 全利用者リスト（ステータス付き） |
| 取得 | 対話ログ一覧取得 | フィルタ条件（任意） | 対話ログリスト |
| 取得 | 対話ログ詳細取得 | 会話ID | 全メッセージと利用者情報 |
| 取得 | 質問傾向分析 | 期間 | よくある質問ランキング、カテゴリ別集計 |
| 取得 | 現在のプロンプト取得 | なし | システムプロンプト + Few-shot例 |
| 更新 | プロンプト更新 | 新しいプロンプトテキスト | 保存完了、バージョン記録 |
| 取得 | ナレッジソース設定取得 | なし | 接続設定、同期状況 |
| 更新 | ナレッジソース設定更新 | 接続情報、対象パス | 設定保存完了 |
| 作成 | 手動再同期実行 | なし | 同期開始→完了通知 |

#### 処理フロー（利用者承認）

1. 管理者が「利用者承認」タブを選択
2. 承認待ち（pending）の利用者一覧を表示
3. 入社リストと照合（名前・所属・入社時期を確認）
4. 承認ボタン → ステータスをapprovedに変更、認証トークンを発行
5. 拒否ボタン → ステータスをrejectedに変更
6. 承認された利用者は次回アクセス時にチャット画面が利用可能に

#### 処理フロー（対話ログ分析）

1. 管理者が「対話ログ分析」タブを選択
2. 全対話ログを新しい順に一覧表示
3. フィルタ条件を設定（所属、入社時期、期間等）
4. 絞り込まれた結果を表示
5. 特定の対話をクリックして詳細展開（質問→回答の全文）

#### 処理フロー（プロンプト設定）

1. 管理者が「プロンプト設定」タブを選択
2. 現在のシステムプロンプトとFew-shot例を表示
3. 編集して保存
4. 変更履歴にバージョンが記録される

#### 処理フロー（ナレッジソース設定）

1. 管理者が「ナレッジソース設定」タブを選択
2. Notion/Google Driveの接続状況と同期状況を表示
3. 接続情報の編集・保存
4. 手動再同期ボタンで即座にナレッジを再取得・インデックス更新

#### データ構造（概念）

```yaml
AdminSession:
  識別子: session_id（UUID）
  基本情報:
    - authenticated（boolean）
  メタ情報:
    - created_at
    - expires_at

PromptConfig:
  識別子: config_id（UUID）
  基本情報:
    - system_prompt（テキスト）
    - few_shot_examples（JSON配列）
    - version（整数）
  メタ情報:
    - created_at
    - updated_by

KnowledgeSource:
  識別子: source_id（UUID）
  基本情報:
    - type（notion / google_drive）
    - connection_config（JSON: トークン、対象パス等）
    - sync_interval_minutes（整数）
  メタ情報:
    - last_synced_at
    - document_count
    - status（active / error / syncing）
```

## 4. データ設計概要

### 4.1 主要エンティティ

```yaml
UserProfile:
  概要: 利用者の基本プロフィール（初回申請 → 管理者承認後に利用可能）
  主要属性:
    - 識別情報: profile_id (UUID)
    - 基本情報: name, department, join_date
    - 承認情報: status (pending/approved/rejected), approved_at, auth_token
    - アクセス情報: last_accessed_at
  関連:
    - Conversation（1対多）

Conversation:
  概要: 1回の対話セッション（複数のメッセージで構成）
  主要属性:
    - 識別情報: conversation_id (UUID)
    - 表示情報: title（自動生成）
  関連:
    - UserProfile（多対1）
    - Message（1対多）

Message:
  概要: 対話内の個別メッセージ（質問または回答）
  主要属性:
    - 識別情報: message_id (UUID)
    - 内容: role (user/assistant), content, input_method (text/voice)
    - RAG情報: retrieved_sources（参照したナレッジソース）
  関連:
    - Conversation（多対1）

PromptConfig:
  概要: AIエージェントのシステムプロンプト設定
  主要属性:
    - 設定: system_prompt, few_shot_examples
    - バージョン: version
  関連: なし（単一設定）

KnowledgeSource:
  概要: ナレッジ接続設定と同期状態
  主要属性:
    - 接続: type (notion/google_drive), connection_config
    - 同期: sync_interval_minutes, last_synced_at, document_count
  関連: なし

KnowledgeChunk:
  概要: チャンク分割されたナレッジドキュメント（ベクトル検索対象）
  主要属性:
    - 内容: content, embedding (vector)
    - メタ: source_type, source_id, source_title, source_url
  関連:
    - KnowledgeSource（多対1）
```

### 4.2 エンティティ関係図

```
UserProfile ──┬── Conversation ──── Message
              │        │
              │        └── (参照) KnowledgeChunk
              │
              │
PromptConfig (独立)

KnowledgeSource ──── KnowledgeChunk（1対多）
```

### 4.3 バリデーションルール

```yaml
UserProfile.name:
  - ルール: 1〜50文字、空文字不可
  - 理由: 対話ログでの利用者識別のため

UserProfile.department:
  - ルール: 1〜100文字、空文字不可
  - 理由: 所属別の傾向分析のため

UserProfile.join_date:
  - ルール: 有効な年月形式（YYYY-MM）
  - 理由: 入社時期別の傾向分析のため

UserProfile.status:
  - ルール: pending / approved / rejected のいずれか
  - 理由: 管理者承認制によるアクセス制御のため

Message.content:
  - ルール: 1〜5000文字、空文字不可
  - 理由: 空の質問防止と過剰入力の制限

AdminPassword:
  - ルール: 8文字以上
  - 理由: 管理画面のセキュリティ確保のため

PromptConfig.system_prompt:
  - ルール: 1〜10000文字、空文字不可
  - 理由: プロンプトの欠落防止

KnowledgeSource.sync_interval_minutes:
  - ルール: 最小15分、最大1440分（24時間）
  - 理由: API Rate Limit保護と鮮度のバランス
```

## 5. 制約事項

### 外部API制限

- **Notion API**: 3リクエスト/秒。大量ページの初期インデックス構築に時間がかかる
- **OpenAI API**: Rate Limit（Tier1: 500RPM）。60名同時利用では問題なし
- **Google Drive API**: 20,000リクエスト/100秒。十分な余裕あり

### 技術的制約

- **音声入力**: Web Speech APIはChrome/Safariのみ対応。非対応ブラウザではWhisper APIにフォールバック
- **Notion API全文取得**: ブロック単位の再帰取得が必要（1リクエスト100ブロック制限）
- **Google Drive Export**: 1ファイル最大10MBのエクスポート制限
- **LLMのハルシネーション**: RAGの検索結果に基づかない回答を生成する可能性あり。「No Absolutes」原則とソース参照で緩和

### スコープ外（作らないもの）

- 完全自律型AI（人の監督を離れて動くシステム）
- 全自動判断（重要な意思決定をAIに任せない）
- システムへの書き込み操作（NO ACTION原則）
- 音声出力（TTS）— MVP対象外、Phase 2で検討
- 外部顧客対応 — MVP対象外、社内利用のみ
- 多言語対応 — MVP対象外

## 6. 複合API処理（バックエンド内部処理）

### 複合処理-001: ナレッジ質問応答

**トリガー**: 利用者がテキストまたは音声で質問を送信
**フロントエンドAPI**: POST /api/chat
**バックエンド内部処理**:

1. （音声の場合）Whisper APIでテキスト変換
2. LlamaIndexでクエリをEmbedding変換
3. pgvectorでベクトル類似検索（Hybrid Search: ベクトル + BM25）
4. 上位5件のナレッジチャンクを取得
5. システムプロンプト + Few-shot例 + 会話履歴 + 検索結果をLLMに送信
6. GPT-4o-miniが安心感のある回答を生成
7. 回答とメタデータ（参照ソース）をDBに保存
8. フロントエンドにストリーミング返却

**結果**: 安心感のあるトーンの回答テキスト + 参照ナレッジソース
**外部サービス依存**: OpenAI API（Whisper + Embedding + GPT-4o-mini）

### 複合処理-002: ナレッジ同期（Notion）

**トリガー**: 定期実行（設定間隔）または管理者の手動再同期
**フロントエンドAPI**: POST /api/admin/sync（手動時）
**バックエンド内部処理**:

1. Notion APIでワークスペース内の対象ページ一覧を取得
2. 各ページの更新日時を確認し、変更があるページを特定
3. 変更ページのブロックを再帰的に全取得
4. テキストを抽出・結合
5. チャンク分割（RecursiveCharacterTextSplitter、400-512トークン）
6. OpenAI Embedding APIでベクトル化
7. pgvectorに保存（既存チャンクは差分更新）

**結果**: ナレッジインデックス更新完了、同期状況をDBに記録
**外部サービス依存**: Notion API、OpenAI Embedding API

### 複合処理-003: ナレッジ同期（Google Drive）

**トリガー**: 定期実行（設定間隔）または管理者の手動再同期
**フロントエンドAPI**: POST /api/admin/sync（手動時）
**バックエンド内部処理**:

1. Google Drive APIで対象フォルダ内のファイル一覧を取得
2. 各ファイルの更新日時を確認し、変更があるファイルを特定
3. Google Docs/Sheets → files.exportでテキスト抽出
4. PDF/Word → ダウンロード後にPythonライブラリでテキスト抽出
5. チャンク分割（RecursiveCharacterTextSplitter、400-512トークン）
6. OpenAI Embedding APIでベクトル化
7. pgvectorに保存（既存チャンクは差分更新）

**結果**: ナレッジインデックス更新完了、同期状況をDBに記録
**外部サービス依存**: Google Drive API、OpenAI Embedding API

## 7. 技術スタック

```yaml
フロントエンド:
  - フレームワーク: React 18
  - 言語: TypeScript 5
  - UIライブラリ: MUI v6
  - 状態管理: Zustand
  - ルーティング: React Router v6
  - データフェッチ: React Query (TanStack Query)
  - ビルドツール: Vite 5

バックエンド:
  - 言語: Python 3.12
  - フレームワーク: FastAPI
  - RAGフレームワーク: LlamaIndex
  - WebSocket: FastAPI WebSocket

データベース:
  - メインDB: Supabase（PostgreSQL + pgvector）

AI/ML:
  - LLM: OpenAI GPT-4o-mini（メイン）
  - Embedding: OpenAI text-embedding-3-small
  - STT: Web Speech API（主）+ OpenAI Whisper API（フォールバック）

インフラ:
  - フロントエンド: Vercel
  - バックエンド: Google Cloud Run
```

## 8. 必要な外部サービス・アカウント

### 必須サービス

| サービス名 | 用途 | 取得先 | 備考 |
|-----------|------|--------|------|
| OpenAI API | LLM回答生成 + Embedding + Whisper STT | platform.openai.com | APIキー発行、従量課金（月$5〜20） |
| Notion API | 社内ナレッジ取得・同期 | notion.so/my-integrations | Internal Integration作成、API無料 |
| Google Cloud | Drive API + Cloud Run | console.cloud.google.com | GCPプロジェクト作成、サービスアカウント発行 |
| Supabase | PostgreSQL + pgvector + ストレージ | supabase.com | プロジェクト作成、無料枠で開始 |
| Vercel | フロントエンドホスティング | vercel.com | GitHub連携、無料枠で開始 |

### オプションサービス

| サービス名 | 用途 | 取得先 | 備考 |
|-----------|------|--------|------|
| Azure Speech Services | TTS音声合成（将来） | azure.microsoft.com | Nanami日本語音声、月50万文字無料 |
| Anthropic Claude API | 高品質LLMフォールバック（将来） | console.anthropic.com | 複雑な質問対応時に検討 |

## 9. 今後の拡張予定

（拡張予定があっても必要最小限の実装）

- Phase 2: 音声出力（TTS）追加、文脈推論の高度化
- Phase 3: 行動できるエージェントへの進化（Read-Only & Safe Actions、Human Escalation）
- Phase 4: 外部CS・多言語展開への拡張
